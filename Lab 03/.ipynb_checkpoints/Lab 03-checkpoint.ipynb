{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Modifying the environment\n",
    "\n",
    "We will make the problem slightly more complicated.\n",
    "\n",
    "The floor is now covered in ice! \n",
    "When an agent makes a movement, it might slip and end up in another cell close to it.\n",
    "##### The probability to arrive in the intended cell is 0.6, and the probability to end up in one of the 4 adjacent cells is 0.1.\n",
    "\n",
    "Similar as for Lab 02, you should create a new Dungeon by inheriting from the original Dungeon environment.\n",
    "##### Again, the step method will return the state instead of observations.\n",
    "And the step function should incorportate these slippery dynamics.\n",
    "\n",
    "##### When the agent slips, rewards accumulate! You could bang your head on the wall twice...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon\n",
    "import numpy as np\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class IceDungeon(Dungeon):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        # In order to explicitely show that the way you represent states doesn't matter, \n",
    "        # we will assign a random index for each coordinate of the grid        \n",
    "        index_states = np.arange(0, N*N)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.coord_to_index_state = index_states.reshape(N,N) #reshape!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        print(self.coord_to_index_state)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        slip_actions = ('stay', 'up', 'down', 'left', 'right')\n",
    "        obs, rew, done, bump = super().step(action)\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "#         print(obs, rew, done, bump, state)\n",
    "#         print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "        \n",
    "        slip_action = np.random.choice(slip_actions, p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "        \n",
    "#         print(slip_action)\n",
    "        if done == False and bump == False and slip_action != 'stay':\n",
    "            obs, rew, done, bump = super().step(slip_action)\n",
    "            \n",
    "            state = self.coord_to_index_state[self.position_agent[0], self.position_agent[1]]\n",
    "#             print(obs, rew, done, bump, state)\n",
    "#             print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "\n",
    "        return obs, rew, done, bump, state\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        super().reset()\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "        #print(self.position_agent)\n",
    "        \n",
    "        return state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21 17 50 59 88 95 22 11 52 75]\n",
      " [65 97 44 47 23 16 85 86 25 40]\n",
      " [41  9 91 84 31 35 56 73 70 90]\n",
      " [87 30 38  4 14 28 37 32 53 74]\n",
      " [43 67 39 78 63 98 89  6 99  8]\n",
      " [61 42 76 93 26 15  0 92 10 51]\n",
      " [68  3 18  7 71 58  2 34 24 96]\n",
      " [77  1 80 94 29 49 48 27 54 19]\n",
      " [69 64 79 83 62 66 81 45 12 36]\n",
      " [46 33 72 13 20 57 60  5 55 82]]\n",
      "X X X X X X X X X X \n",
      "X . . . X . . . . X \n",
      "X X . X . . X . . X \n",
      "X L . . . L . . . X \n",
      "X . . . . . . . L X \n",
      "X . . A . . . . . X \n",
      "X L . . . . . . . X \n",
      "X . . . . . . L . X \n",
      "X . . . . . X E . X \n",
      "X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dungeon = IceDungeon(10)\n",
    "dungeon.reset()\n",
    "dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dungeon.step('up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Monte-carlo learning\n",
    "\n",
    "We will use a simple random policy function to evaluate the state values. \n",
    "\n",
    "First of all, create a random policy function that just picks random actions (check Lab 01).\n",
    "\n",
    "Then, implement a MC learning class that allows to learn the values based on full rollouts of the policy in the environment.\n",
    "\n",
    "Finally, you can generate rollouts of your policy in an environment, and update the values using MC-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    \n",
    "    return np.random.choice(['up', 'down', 'left', 'right'])\n",
    "    \n",
    "class MC_Learning():\n",
    "    \n",
    "    def __init__(self, N, environment, policy, gamma, episodes):\n",
    "        self.N = N\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        self.values = {}\n",
    "    \n",
    "    def generate_episode(self): #, rollout\n",
    "        \n",
    "        state = self.environment.reset()\n",
    "        \n",
    "        states_and_rewards = [(state, 0)]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.policy()\n",
    "            obs, rew, done, bump, state = self.environment.step(action)\n",
    "            #print(action,rew,state)\n",
    "            states_and_rewards.append((state,rew))\n",
    "            #total_reward += reward     \n",
    "        #print(states_and_rewards)\n",
    "        #print(done)\n",
    "        \n",
    "        G = 0\n",
    "        states_and_returns = []\n",
    "        first = True\n",
    "        for state, rew in reversed(states_and_rewards):\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                states_and_returns.append((state,G))\n",
    "            G = rew + self.gamma * G\n",
    "        states_and_returns.reverse()\n",
    "        #print(states_and_returns)\n",
    "        \n",
    "        return states_and_returns\n",
    "    \n",
    "    def update_values(self): #, rollout\n",
    "        \n",
    "        # print(self.values)\n",
    "        returns = {}\n",
    "        states = self.environment.coord_to_index_state.reshape(self.N*self.N)\n",
    "        \n",
    "        for s in states:\n",
    "            returns[s] = []\n",
    "        print(returns)\n",
    "            \n",
    "        for episode in range(self.episodes):\n",
    "            states_and_returns = self.generate_episode()\n",
    "            seen_states = set()\n",
    "            #print(states_and_returns)\n",
    "            for state, G in states_and_returns:\n",
    "                #print(state,G)\n",
    "                if state not in seen_states:\n",
    "                    returns[state].append(G)\n",
    "                    self.values[state] = np.mean(returns[state])\n",
    "                    seen_states.add(state)\n",
    "        print(self.values)\n",
    "            \n",
    "    def display_values(self):\n",
    "        display = []\n",
    "        for row in self.environment.coord_to_index_state:\n",
    "            for col in row:\n",
    "                if col == self.values[col]:\n",
    "                    print(self.values[col])\n",
    "#         display = []\n",
    "#         for i in self.environment.coord_to_index_state.reshape(self.N*self.N):\n",
    "#             print(self.values[i])\n",
    "#             if i == self.values[i]:\n",
    "#                 display.append(self.values[i])\n",
    "#             else:\n",
    "#                 display.append(0)\n",
    "#         print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 89  9 58 36 99 30  6  3  7]\n",
      " [43 28 14 24 88 79 97 39 40 52]\n",
      " [34 59 19 31 94 18 45 47 64 70]\n",
      " [32 71 77 38 63  1 27 60 92 26]\n",
      " [73 44 55 91 65 46 25 61 85 69]\n",
      " [21  2 54 98 20 78 84 68 35 16]\n",
      " [49 37 50 96 29 81 15 11 82 51]\n",
      " [33 86 90  4 67 74 66 12 87  5]\n",
      " [42 83 53 93 72 76 22 57 75  8]\n",
      " [56 62 80 41 10  0 23 48 17 95]]\n",
      "X X X X X X X X X X \n",
      "X . . . . . . . . X \n",
      "X . . . X . . . . X \n",
      "X L . . . . . . . X \n",
      "X . . L . . . . L X \n",
      "X . . . . X . . . X \n",
      "X . E L . . . X L X \n",
      "X A . . . . . . . X \n",
      "X . X X . . . . . X \n",
      "X X X X X X X X X X \n",
      "\n",
      "{13: [], 89: [], 9: [], 58: [], 36: [], 99: [], 30: [], 6: [], 3: [], 7: [], 43: [], 28: [], 14: [], 24: [], 88: [], 79: [], 97: [], 39: [], 40: [], 52: [], 34: [], 59: [], 19: [], 31: [], 94: [], 18: [], 45: [], 47: [], 64: [], 70: [], 32: [], 71: [], 77: [], 38: [], 63: [], 1: [], 27: [], 60: [], 92: [], 26: [], 73: [], 44: [], 55: [], 91: [], 65: [], 46: [], 25: [], 61: [], 85: [], 69: [], 21: [], 2: [], 54: [], 98: [], 20: [], 78: [], 84: [], 68: [], 35: [], 16: [], 49: [], 37: [], 50: [], 96: [], 29: [], 81: [], 15: [], 11: [], 82: [], 51: [], 33: [], 86: [], 90: [], 4: [], 67: [], 74: [], 66: [], 12: [], 87: [], 5: [], 42: [], 83: [], 53: [], 93: [], 72: [], 76: [], 22: [], 57: [], 75: [], 8: [], 56: [], 62: [], 80: [], 41: [], 10: [], 0: [], 23: [], 48: [], 17: [], 95: []}\n",
      "{76: -27.763688279078835, 67: -12.535420162750833, 72: -22.583330614418962, 74: -11.52097421824134, 81: -21.793445160887444, 15: -10.643320402507987, 66: -21.782673060002523, 29: -2.3916852781219227, 4: -7.128686049643894, 12: -11.720026267102192, 57: -34.89347295622616, 22: -27.534216238034787, 84: -13.674741230797741, 68: -10.871174503460834, 35: -86.02026456444435, 25: -31.371009722875886, 61: -20.62997917585396, 60: -27.182387021678785, 92: -68.10844918066931, 82: -102.41309346124504, 85: -13.624065062570299, 96: 50.09867123313626, 27: -17.511656711225157, 1: -7.595235429414913, 63: -7.435804624387752, 38: -25.642427352546573, 24: -28.113283545264952, 14: -28.53208148689014, 31: -23.28060653105004, 65: -5.272139016374246, 20: -20.850906836172307, 91: -26.574750261850433, 77: -25.48444354056679, 98: 47.72749394500001, 55: 9.555032612002964, 44: -1.5322715086909369, 90: -13.95093855076132, 86: -5.748955186566668, 83: 41.21841000000001, 46: -1.8560727382530562, 18: -29.30155251007872, 45: -21.816740872119716, 97: -23.129712080133018, 79: -24.588568977925576, 47: -21.417030999999998, 87: 0.8549918836664219, 2: 0.5394446142834427, 19: -23.035563371914225, 59: -23.06907134992958, 28: -26.209627219293676, 71: -20.60774871666681, 54: -12.12029004927079, 75: -51.85116735943812}\n"
     ]
    }
   ],
   "source": [
    "dungeon = IceDungeon(10)\n",
    "dungeon.reset()\n",
    "dungeon.display()\n",
    "test = MC_Learning(10, dungeon, random_policy, 0.9, 10)\n",
    "test.update_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X \n",
      "X . A . . . . . . X \n",
      "X . . . X . . . . X \n",
      "X L . . . . . . . X \n",
      "X . . L . . . . L X \n",
      "X . . . . X . . . X \n",
      "X . E L . . . X L X \n",
      "X . . . . . . . . X \n",
      "X . X X . . . . . X \n",
      "X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "13",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-344-72f288eec5a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-341-e6c5169e38d1>\u001b[0m in \u001b[0;36mdisplay_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoord_to_index_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#         display = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 13"
     ]
    }
   ],
   "source": [
    "test.display_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: TD-learning\n",
    "\n",
    "We will now compare the values obtained by MC learning with values obtained with TD learning.\n",
    "\n",
    "Create a class that allows to update values every time a new state transition occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_Learning():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        self.values = ...\n",
    "    \n",
    "    def update_values(self, s_current, reward_next, s_next):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
