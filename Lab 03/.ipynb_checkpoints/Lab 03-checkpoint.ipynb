{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Modifying the environment\n",
    "\n",
    "We will make the problem slightly more complicated.\n",
    "\n",
    "The floor is now covered in ice! \n",
    "When an agent makes a movement, it might slip and end up in another cell close to it.\n",
    "##### The probability to arrive in the intended cell is 0.6, and the probability to end up in one of the 4 adjacent cells is 0.1.\n",
    "\n",
    "Similar as for Lab 02, you should create a new Dungeon by inheriting from the original Dungeon environment.\n",
    "##### Again, the step method will return the state instead of observations.\n",
    "And the step function should incorportate these slippery dynamics.\n",
    "\n",
    "##### When the agent slips, rewards accumulate! You could bang your head on the wall twice...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon\n",
    "import numpy as np\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class IceDungeon(Dungeon):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        # In order to explicitely show that the way you represent states doesn't matter, \n",
    "        # we will assign a random index for each coordinate of the grid        \n",
    "        index_states = np.arange(0, N*N)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.coord_to_index_state = index_states.reshape(N,N) #reshape!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        print(self.coord_to_index_state)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        slip_actions = ('stay', 'up', 'down', 'left', 'right')\n",
    "        obs, rew, done, bump = super().step(action)\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "#         print(obs, rew, done, bump, state)\n",
    "#         print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "        \n",
    "        slip_action = np.random.choice(slip_actions, p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "        \n",
    "#         print(slip_action)\n",
    "        if done == False and bump == False and slip_action != 'stay':\n",
    "            obs, rew, done, bump = super().step(slip_action)\n",
    "            \n",
    "            state = self.coord_to_index_state[self.position_agent[0], self.position_agent[1]]\n",
    "#             print(obs, rew, done, bump, state)\n",
    "#             print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "\n",
    "        return obs, rew, done, bump, state\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        super().reset()\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "        #print(self.position_agent)\n",
    "        \n",
    "        return state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 60 92  9 37 84 74 89 36  2]\n",
      " [77 22 46 28 72 93 75 86 23 79]\n",
      " [85 98  5 54  4 81 11  8 83 57]\n",
      " [16 73 58 13 56 20 99 68 32 41]\n",
      " [43 50 88 17 42 18 52 91 29 76]\n",
      " [27  3 55 95 10 64 62 35 61 80]\n",
      " [38 63 78 34 21 97  7 49 66  0]\n",
      " [59 71 30  1 12 51 82 26 69 39]\n",
      " [87 70 94 31 44 40 67 24  6 19]\n",
      " [33 96 53 45 48 14 47 90 25 65]]\n",
      "X X X X X X X X X X \n",
      "X . . . . . . L . X \n",
      "X . . . . X . X . X \n",
      "X . . . A . . . . X \n",
      "X . . . . . . . . X \n",
      "X . . . . . L L . X \n",
      "X . . . . . . X X X \n",
      "X . . L . E . L . X \n",
      "X . . . . . X . . X \n",
      "X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dungeon = IceDungeon(10)\n",
    "dungeon.reset()\n",
    "dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dungeon.step('up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Monte-carlo learning\n",
    "\n",
    "We will use a simple random policy function to evaluate the state values. \n",
    "\n",
    "First of all, create a random policy function that just picks random actions (check Lab 01).\n",
    "\n",
    "Then, implement a MC learning class that allows to learn the values based on full rollouts of the policy in the environment.\n",
    "\n",
    "Finally, you can generate rollouts of your policy in an environment, and update the values using MC-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    \n",
    "    return np.random.choice(['up', 'down', 'left', 'right'])\n",
    "    \n",
    "class MC_Learning():\n",
    "    \n",
    "    def __init__(self, N, environment, policy, gamma, episodes):\n",
    "        self.N = N\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        self.values = {}\n",
    "    \n",
    "#     def run_single_exp(envir, policy):\n",
    "    \n",
    "#         obs = envir.reset()\n",
    "#         done = False\n",
    "    \n",
    "#         total_reward = 0\n",
    "    \n",
    "#         while not done:\n",
    "#             action = policy(obs)\n",
    "#             obs, reward, done = envir.step(action)\n",
    "#             total_reward += reward\n",
    "        \n",
    "#         return total_reward\n",
    "    \n",
    "#     def generate_episodes(self):\n",
    "#         self.policy\n",
    "#         G = 0\n",
    "#         states_and_returns = []\n",
    "#         first = True\n",
    "#         for s, r in reversed(states_and_rewards):\n",
    "#             print('11111111',s, r)\n",
    "#             # the value of the terminal state is 0 by definition\n",
    "#             # we should ignore the first state we encounter\n",
    "#             # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "#         if first:\n",
    "#             first = False\n",
    "#         else:\n",
    "#             states_and_returns.append((s, G))\n",
    "#         G = r + GAMMA*G\n",
    "#         states_and_returns.reverse() # we want it to be in order of state visited\n",
    "#         return states_and_returns\n",
    "    \n",
    "    def generate_episode(self): #, rollout\n",
    "        \n",
    "        state = self.environment.reset()\n",
    "#         self.environment.display()#####################################################\n",
    "        \n",
    "        states_and_rewards = [(state, 0)]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.policy()\n",
    "            obs, rew, done, bump, state = self.environment.step(action)\n",
    "            #print(action,rew,state)\n",
    "            states_and_rewards.append((state,rew))\n",
    "            #total_reward += reward     \n",
    "        #print(states_and_rewards)\n",
    "        #print(done)\n",
    "        \n",
    "        G = 0\n",
    "        states_and_returns = []\n",
    "        first = True\n",
    "        for state, rew in reversed(states_and_rewards):\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                states_and_returns.append((state,G))\n",
    "            G = rew + self.gamma * G\n",
    "        states_and_returns.reverse()\n",
    "        #print(states_and_returns)\n",
    "        \n",
    "        return states_and_returns\n",
    "    \n",
    "    def update_values(self): #, rollout\n",
    "        \n",
    "        # print(self.values)\n",
    "        returns = {}\n",
    "        states = self.environment.coord_to_index_state.reshape(self.N*self.N)\n",
    "        \n",
    "        for s in states:\n",
    "            returns[s] = []\n",
    "            \n",
    "        for episode in range(self.episodes):\n",
    "            states_and_returns = self.generate_episode()\n",
    "            seen_states = set()\n",
    "            #print(states_and_returns)\n",
    "            for state, G in states_and_returns:\n",
    "                #print(state,G)\n",
    "                if state not in seen_states:\n",
    "                    returns[state].append(G)\n",
    "                    self.values[state] = np.mean(returns[state])\n",
    "                    seen_states.add(state)\n",
    "        print(self.values)\n",
    "            \n",
    "    def display_values(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87 93 16 18 12 81 28 76 98 38]\n",
      " [99 36 35 95 68 90 82 92 10 86]\n",
      " [48 88 52 96 60 46  2 50 45 78]\n",
      " [21 33 94 27 64 77 97 54 19 30]\n",
      " [75 49 63 62 51 59 67  4 37 25]\n",
      " [39 43 61 66 91 70  9  1 71 34]\n",
      " [73 83 80 89 32 26  6 40 65 58]\n",
      " [31 22 14 24 42 74 57 55  3 85]\n",
      " [ 5 72 13 84 44 79 23 17  0 20]\n",
      " [29  8 69 47 11 56 53 41  7 15]]\n"
     ]
    }
   ],
   "source": [
    "dungeon = IceDungeon(10)\n",
    "test = MC_Learning(10, dungeon, random_policy, 0.9, 9999)\n",
    "test.update_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: TD-learning\n",
    "\n",
    "We will now compare the values obtained by MC learning with values obtained with TD learning.\n",
    "\n",
    "Create a class that allows to update values every time a new state transition occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_Learning():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        self.values = ...\n",
    "    \n",
    "    def update_values(self, s_current, reward_next, s_next):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
