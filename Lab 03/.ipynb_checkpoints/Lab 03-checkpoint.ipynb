{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Modifying the environment\n",
    "\n",
    "We will make the problem slightly more complicated.\n",
    "\n",
    "The floor is now covered in ice! \n",
    "When an agent makes a movement, it might slip and end up in another cell close to it.\n",
    "##### The probability to arrive in the intended cell is 0.6, and the probability to end up in one of the 4 adjacent cells is 0.1.\n",
    "\n",
    "Similar as for Lab 02, you should create a new Dungeon by inheriting from the original Dungeon environment.\n",
    "##### Again, the step method will return the state instead of observations.\n",
    "And the step function should incorportate these slippery dynamics.\n",
    "\n",
    "##### When the agent slips, rewards accumulate! You could bang your head on the wall twice...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IceDungeon(Dungeon):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        # In order to explicitely show that the way you represent states doesn't matter, \n",
    "        # we will assign a random index for each coordinate of the grid        \n",
    "        index_states = np.arange(0, N*N)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.coord_to_index_state = index_states.reshape(N,N) #reshape!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        print(self.coord_to_index_state)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        slip_actions = ('stay', 'up', 'down', 'left', 'right')\n",
    "        obs, rew, done, bump = super().step(action)\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "        print(obs, rew, done, bump, state)\n",
    "        print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "        \n",
    "        slip_action = np.random.choice(slip_actions, p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "        \n",
    "        print(slip_action)\n",
    "        if done == False and bump == False and slip_action != 'stay':\n",
    "            obs, rew, done, bump = super().step(slip_action)\n",
    "            \n",
    "            state = self.coord_to_index_state[self.position_agent[0], self.position_agent[1]]\n",
    "            print(obs, rew, done, bump, state)\n",
    "            print(self.position_agent, 'time escape', self.time_elapsed)\n",
    "\n",
    "        return obs, rew, done, bump, state\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        super().reset()\n",
    "        state = self.coord_to_index_state[ self.position_agent[0], self.position_agent[1]]\n",
    "        print(state)\n",
    "        print(self.position_agent)\n",
    "        \n",
    "        return state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75 71 65 53 35 60 77 12 24 63]\n",
      " [49 30  3 41 55 51 82 74  9 17]\n",
      " [61  5 14 52 48 97 92 83 88 31]\n",
      " [28 23 79 29 46 99 27 54 19 91]\n",
      " [89 86  8  2 22 87 11 40 85 81]\n",
      " [36 84 38 16 21 15 42 43 76 13]\n",
      " [72  4 10 94 62  6 26 47 50 57]\n",
      " [20 67 44 68 45 59 66 73 95 64]\n",
      " [93 58 96 98  7 18 39 33 90  1]\n",
      " [37 70 34  0 25 80 56 69 32 78]]\n",
      "92\n",
      "[2 6]\n",
      "0\n",
      "X X X X X X X X X X \n",
      "X L . . . L . . X X \n",
      "X . . . L . A . . X \n",
      "X . . . X . . . . X \n",
      "X . . . . . . X X X \n",
      "X . . . . . . . . X \n",
      "X . . . . . E L . X \n",
      "X . . . . . . . . X \n",
      "X . L . . . X . . X \n",
      "X X X X X X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dungeon = IceDungeon(10)\n",
    "dungeon.reset()\n",
    "dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relative_coordinates': array([5, 0], dtype=int64), 'surroundings': array([[1, 1, 1],\n",
      "       [2, 0, 0],\n",
      "       [0, 0, 0]], dtype=int8)} -1 False False 82\n",
      "[1 6] 1\n",
      "left\n",
      "{'relative_coordinates': array([5, 1], dtype=int64), 'surroundings': array([[1, 1, 1],\n",
      "       [0, 2, 0],\n",
      "       [2, 0, 0]], dtype=int8)} -21 False False 51\n",
      "[1 5] 2\n"
     ]
    }
   ],
   "source": [
    "a = dungeon.step('up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "index_states = np.arange(0, N*N)\n",
    "#np.random.shuffle(index_states)\n",
    "index_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Monte-carlo learning\n",
    "\n",
    "We will use a simple random policy function to evaluate the state values. \n",
    "\n",
    "First of all, create a random policy function that just picks random actions (check Lab 01).\n",
    "\n",
    "Then, implement a MC learning class that allows to learn the values based on full rollouts of the policy in the environment.\n",
    "\n",
    "Finally, you can generate rollouts of your policy in an environment, and update the values using MC-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \n",
    "    return random.choice(['up', 'down', 'left', 'right'])\n",
    "    \n",
    "class MC_Learning():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        self.values = ...\n",
    "    \n",
    "    def update_values(self, rollout):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: TD-learning\n",
    "\n",
    "We will now compare the values obtained by MC learning with values obtained with TD learning.\n",
    "\n",
    "Create a class that allows to update values every time a new state transition occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_Learning():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        self.values = ...\n",
    "    \n",
    "    def update_values(self, s_current, reward_next, s_next):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
