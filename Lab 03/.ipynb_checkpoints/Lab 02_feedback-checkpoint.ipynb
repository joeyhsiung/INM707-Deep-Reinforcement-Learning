{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Modifying the environment\n",
    "\n",
    "In order to use RL and Dynamic programming, the agent needs to know its state.\n",
    "In the general case, it needs to estimate its state, however for the first part of the lecture we will simply provide the state to the agent.\n",
    "\n",
    "In the case of our Dungeon environment, the state of the agent is entirely described by either the coordinates of the cell it is on, or the index of the cell.\n",
    "\n",
    "The first task is to modify the environment so that we can get all the necessary information:\n",
    "- the value function (can be represented as a dictionary, or an array) that maps the state to the value of the state. As we don't know the true value at the beginning, it will be initialized at 1.\n",
    "- the transition matrix, which maps the state and actions to new states\n",
    "- the reward matrix, which maps the state and action to rewards\n",
    "\n",
    "In the solutions of this lab, we will use the index of the cell as the state.\n",
    "\n",
    "These can be calculated once an environment is instantiated.\n",
    "If the environment doesn't change after a reset, it doesn't need to be calculated again.\n",
    "\n",
    "Next, we also need to adapt our step function, to return the state instead of our observations.\n",
    "\n",
    "In order to modify the environment, a convenient way is to inherit our original Dungeon Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "# COnvenient data structure to hold information about actions\n",
    "Action = namedtuple('Action', 'name index delta_i delta_j')\n",
    "    \n",
    "up = Action('up', 0, -1, 0)    \n",
    "down = Action('down', 1, 1, 0)    \n",
    "left = Action('left', 2, 0, -1)    \n",
    "right = Action('right', 3, 0, 1)    \n",
    "\n",
    "index_to_actions = {}\n",
    "for action in [up, down, left, right]:\n",
    "    index_to_actions[action.index] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DungeonDP(Dungeon):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        # Additional calculations to get the transition and reward matrix\n",
    "        self.reward_matrix = -1 * np.ones((N*N, 4)) # All movements give you by default a reward of -1\n",
    "        self.transition_matrix = np.zeros((N*N, 4, N*N))\n",
    "        \n",
    "        # In order to explicitely show that the way you represent states doesn't matter, \n",
    "        # we will assign a random index for each coordinate of the grid        \n",
    "        index_states = np.arange(0, N*N)\n",
    "        np.random.shuffle(index_states)\n",
    "        self.coord_to_index_state = index_states.reshape(N,N)\n",
    "        \n",
    "        # fill the matrix with appropriate values\n",
    "        # What happens for cells corresponding to obstacles? what should they transition to?\n",
    "        for i in range(1, N-1):\n",
    "            for j in range(1, N-1):\n",
    "                \n",
    "                current_state = self.coord_to_index_state[i,j]\n",
    "                current_cell = self.dungeon[i,j]\n",
    "                \n",
    "                for action in [up, left, down, right]:\n",
    "                    \n",
    "                    destination_cell = self.dungeon[i +action.delta_i , j + action.delta_j]\n",
    "                    next_state = self.coord_to_index_state[i + action.delta_i, j + action.delta_j]\n",
    "\n",
    "                    # Check if you bump\n",
    "                    if destination_cell in [0, 2, 3]:    \n",
    "                        self.transition_matrix[current_state, action.index, next_state] = 1\n",
    "                    else:\n",
    "                        self.transition_matrix[current_state, action.index, current_state] = 1\n",
    "                        destination_cell = current_cell\n",
    "                        next_state = current_state\n",
    "                        self.reward_matrix[current_state, action.index] += -5 \n",
    "\n",
    "                    # Check where the agent ends up\n",
    "                    if destination_cell == 2:\n",
    "                        self.reward_matrix[current_state, action.index] += -20\n",
    "                    elif destination_cell == 3:\n",
    "                        self.reward_matrix[current_state, action.index] += N**2\n",
    "                        \n",
    "    def print_reward_matrices(self):\n",
    "        \n",
    "        for action in [up, left, down, right]:\n",
    "            \n",
    "#             reward_matrix = self.reward_matrix[:,action.index].reshape(self.size, self.size)\n",
    "\n",
    "            reward_matrix = np.zeros( (self.size, self.size) )\n",
    "    \n",
    "            for i in range(1, self.size-1):\n",
    "                    for j in range(1, self.size-1):\n",
    "\n",
    "                        state = self.coord_to_index_state[i, j]\n",
    "                        reward_matrix[i,j] = self.reward_matrix[state, action.index]\n",
    "                        \n",
    "            print( action.name)\n",
    "            print(reward_matrix)\n",
    "            \n",
    "        \n",
    "#     def step(action):\n",
    "        \n",
    "#         obs, rew, done = super().step(action)\n",
    "        \n",
    "#         state = ...\n",
    "        \n",
    "#         return state, rew, done\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X \n",
      "X X L X X \n",
      "X . L . X \n",
      "X E . A X \n",
      "X X X X X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dungeon = DungeonDP(5)\n",
    "dungeon.reset()\n",
    "\n",
    "dungeon.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.  -6. -26.  -6.   0.]\n",
      " [  0.  -6. -21.  -6.   0.]\n",
      " [  0.  -1. -21.  -1.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "left\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.  -6. -26. -21.   0.]\n",
      " [  0.  -6.  -1. -21.   0.]\n",
      " [  0.  19.  24.  -1.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "down\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.  -1. -21.  -1.   0.]\n",
      " [  0.  24.  -1.  -1.   0.]\n",
      " [  0.  19.  -6.  -6.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "right\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0. -21. -26.  -6.   0.]\n",
      " [  0. -21.  -1.  -6.   0.]\n",
      " [  0.  -1.  -1.  -6.   0.]\n",
      " [  0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "dungeon.print_reward_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Policy Evaluation\n",
    "\n",
    "We will choose a random policy as initial policy.\n",
    "\n",
    "Create a class Policy which is initialized as a random policy. (Each action can be picked with a probability of 0.25 for every state).\n",
    "\n",
    "When calling the instance of a Policy, with a state as argument, this should return the selected action. \n",
    "\n",
    "We will also implement a method for policy evaluation, and a method for policy improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self, environment, gamma):\n",
    "        \n",
    "        self.size_environment = environment.size\n",
    "        self.transition_matrix = environment.transition_matrix\n",
    "        self.reward_matrix = environment.reward_matrix\n",
    "        \n",
    "        self.size_state_space = self.dungeon.size_envir**2\n",
    "        \n",
    "        # the policy is a numpy array that collects all the probabilities of chosing an action when in a state\n",
    "        self.probability_actions = np.ones((self.size_state_space, 4))*0.25        \n",
    "        \n",
    "        # We initialize the values to 0\n",
    "        self.values = np.zeros( self.size_state_space )\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        # Sample an action from the policy, given a state\n",
    "        \n",
    "        proba_of_action = self.probability_actions[state]\n",
    "        index_action = np.random.multinomial(1, proba_of_action )\n",
    "        \n",
    "        return actions[index_action].name\n",
    "        \n",
    "    def iterative_policy_evaluation( self, n_iterations):\n",
    "    \n",
    "        # restart from initial values\n",
    "        # we could restart also from random values, or from our previous estimate of values\n",
    "        \n",
    "        self.values = np.zeros( self.size_state_space )        \n",
    "\n",
    "        # We update the values using the bellman equation\n",
    "        for iteration in range(n_iterations):\n",
    "            self.values = np.sum(self.probability_actions * \n",
    "                                 (self.dungeon.reward_matrix \n",
    "                                  + self.gamma * np.dot(self.dungeon.state_transition_matrix, self.values) \n",
    "                                 ), axis = 1) \n",
    "    \n",
    "    def display_values(self):\n",
    "        \n",
    "        value_matrix = np.zeros( (self.dungeon.size_envir, self.dungeon.size_envir) )\n",
    "    \n",
    "        for i in range(self.dungeon.size):\n",
    "            for j in range(self.dungeon.size):\n",
    "\n",
    "                state = index_to_coord[i, j]\n",
    "                value_matrix[i,j] = values[state]\n",
    "                                \n",
    "        print(value_matrix)\n",
    "        \n",
    "    def greedy_improvement(values):\n",
    "    \n",
    "        self.probability_actions\n",
    "    \n",
    "#         self.policy = new_policy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DungeonDP' object has no attribute 'size_envir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-4073cacc431c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdungeon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterative_policy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdungeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoord_to_index_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-2e56a60cc615>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dungeon, gamma)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdungeon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdungeon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_state_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdungeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_envir\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# the policy is a numpy array that collects all the probabilities of chosing an action when in a state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DungeonDP' object has no attribute 'size_envir'"
     ]
    }
   ],
   "source": [
    "policy = Policy(dungeon, gamma=0.3)\n",
    "\n",
    "values = policy.iterative_policy_evaluation( 10)\n",
    "\n",
    "policy.display_values(values, dungeon.coord_to_index_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Now that you have both policy evaluation and improvement, you can interatively perform evaluation and improvement.\n",
    "\n",
    "Calculate, after each improvement, how well the policy is doing. Maybe add some plots to see how it performs depending on parameters gamma and k (number of steps for policy evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import run_experiments\n",
    "\n",
    "dungeon = dungeonDP(10)\n",
    "gamma = 0.2\n",
    "\n",
    "policy = Policy(10)\n",
    "\n",
    "for n_improvements in range(100):\n",
    "    \n",
    "    values = policy.iterative_policy_evaluation(reward_matrix, state_transition_matrix, gamma)\n",
    "    policy.greedy_improvement(values)\n",
    "    \n",
    "    _, max_reward, mean_reward, var_reward = run_experiments(dungeon, policy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
