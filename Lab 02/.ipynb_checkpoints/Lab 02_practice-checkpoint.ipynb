{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Modifying the environment\n",
    "\n",
    "In order to use RL and Dynamic programming, the agent needs to know its state.\n",
    "In the general case, it needs to estimate its state, however for the first part of the lecture we will simply provide the state to the agent.\n",
    "\n",
    "In the case of our Dungeon environment, the state of the agent is entirely described by either the coordinates of the cell it is on, or the index of the cell.\n",
    "\n",
    "The first task is to modify the environment so that we can get all the necessary information:\n",
    "- the value function (can be represented as a dictionary, or an array) that maps the state to the value of the state. As we don't know the true value at the beginning, it will be initialized at 1.\n",
    "- the transition matrix, which maps the state and actions to new states\n",
    "- the reward matrix, which maps the state and action to rewards\n",
    "\n",
    "In the solutions of this lab, we will use the index of the cell as the state.\n",
    "\n",
    "These can be calculated once an environment is instantiated.\n",
    "If the environment doesn't change after a reset, it doesn't need to be calculated again.\n",
    "\n",
    "Next, we also need to adapt our step function, to return the state instead of our observations.\n",
    "\n",
    "In order to modify the environment, a convenient way is to inherit our original Dungeon Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import Dungeon\n",
    "import numpy as np\n",
    "class DungeonDP(Dungeon):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        super().__init__(N)\n",
    "        \n",
    "        # Additional calculations to get the transition and reward matrix\n",
    "        self.reward_matrix = np.zeros( (N*N, 4, 1) )\n",
    "        self.state_transition_matrix = np.zeros( (N*N, 4, N*N) )\n",
    "        self.value_function = np.ones((N*N))\n",
    "        print(self.dungeon)\n",
    "        \n",
    "#         ... # fill the matrix with appropriate values\n",
    "#         Index = {}\n",
    "#         begin = 0\n",
    "#         for Y in range(N):\n",
    "#             for X in range(N):\n",
    "#                 Index[begin] = [Y,X]\n",
    "#                 begin += 1\n",
    "#         print(Index)\n",
    "        \n",
    "#         print(self.dungeon)\n",
    "#         for t in range(N*N):\n",
    "#             for t1 in range(N*N):\n",
    "#                 for pro in range(4):\n",
    "#                     #dungeon[Y][X]\n",
    "#                     pass\n",
    "        \n",
    "        # What happens for cells corresponding to obstacles? what should they transition to?\n",
    "        \n",
    "    def step1(action):\n",
    "        \n",
    "        obs, rew, done = super().step(action)\n",
    "        print('obs',obs)\n",
    "        \n",
    "        state = obs\n",
    "        \n",
    "        return state, rew, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 1]\n",
      " [1 2 1 0 0 0 1 0 0 1]\n",
      " [1 0 0 2 1 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 1]\n",
      " [1 2 0 1 0 0 2 0 0 1]\n",
      " [1 0 0 1 0 0 0 0 0 1]\n",
      " [1 0 2 0 0 0 0 0 0 1]\n",
      " [1 0 3 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n",
      "X X X X X X X X X X \n",
      "X . . . . . . . . X \n",
      "X L X . . . X . . X \n",
      "X . . L X . . . . X \n",
      "X . . A . . . . . X \n",
      "X L . X . . L . . X \n",
      "X . . X . . . . . X \n",
      "X . L . . . . . . X \n",
      "X . E . . . . . . X \n",
      "X X X X X X X X X X \n",
      "\n",
      "1 {'relative_coordinates': array([ 5, -1], dtype=int64), 'surroundings': array([[1, 0, 0],\n",
      "       [0, 2, 1],\n",
      "       [0, 0, 0]], dtype=int8)} 2 -21 3 False\n"
     ]
    }
   ],
   "source": [
    "dungeon = DungeonDP(10)\n",
    "dungeon.reset()\n",
    "dungeon.display()\n",
    "state, rew, done = dungeon.step('up')\n",
    "# relative_coordinates = position_exit - position_agent\n",
    "print('1',state,'2', rew, '3', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Policy Evaluation\n",
    "\n",
    "We will choose a random policy as initial policy.\n",
    "\n",
    "Create a class Policy which is initialized as a random policy. (Each action can be picked with a probability of 0.25 for every state).\n",
    "\n",
    "When calling the instance of a Policy, with a state as argument, this should return the selected action. \n",
    "\n",
    "We will also implement a method for policy evaluation, and a method for policy improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        # the policy is a numpy array that collects all the probabilities of chosing an action when in a state\n",
    "        self.policy = np.ones((N*N,4))/4\n",
    "        self.N = N\n",
    "        self.actions = ['up','down', 'left', 'right']\n",
    "        \n",
    "    \n",
    "    def __call__(self):\n",
    "        \n",
    "        # When called, the instance of Policy will return the selected action, sampled from the probability of !!!!!!!!!ACTIONS„ÄÅ!!!!!!!!!!!\n",
    "        return action\n",
    "        \n",
    "    def iterative_policy_evaluation( self, reward_matrix, state_transition_matrix, gamma):\n",
    "    \n",
    "        values = np.zeros(self.N*self.N) # initialize values to 0\n",
    "    \n",
    "        for iteration in range(k):\n",
    "        \n",
    "            # update values\n",
    "        \n",
    "        return values\n",
    "        \n",
    "        \n",
    "    def greedy_improvement(values):\n",
    "    \n",
    "        new policy = np.zeros(self.policy.shape)\n",
    "    \n",
    "        self.policy = new_policy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Now that you have both policy evaluation and improvement, you can interatively perform evaluation and improvement.\n",
    "\n",
    "Calculate, after each improvement, how well the policy is doing. Maybe add some plots to see how it performs depending on parameters gamma and k (number of steps for policy evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dungeon.dungeon import run_experiments\n",
    "\n",
    "dungeon = dungeonDP(10)\n",
    "gamma = 0.2\n",
    "\n",
    "policy = Policy(10)\n",
    "\n",
    "for n_improvements in range(100):\n",
    "    \n",
    "    values = policy.iterative_policy_evaluation(reward_matrix, state_transition_matrix, gamma)\n",
    "    policy.greedy_improvement(values)\n",
    "    \n",
    "    _, max_reward, mean_reward, var_reward = run_experiments(dungeon, policy)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
