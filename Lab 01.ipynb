{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 - Exit the dungeon!\n",
    "\n",
    "In this first lab, we will create by hand our first Reinforcement Learning environment.\n",
    "A lot of agents will be harmed in the process of solving the lab.\n",
    "\n",
    "## The environment\n",
    "\n",
    "The environment is a NxN array of integers. \n",
    "Each cell of this environment can have the following values:\n",
    "- 0 : empty cell\n",
    "- 1 : obstacle, non-traversable\n",
    "- 2 : lava\n",
    "- 3 : exit\n",
    "\n",
    "All border cells are obstacles.\n",
    "Upon initialization, the environment has:\n",
    "- N/2 obstacles placed randomly in the maze.\n",
    "- N/2 lava cells placed randomly in the cell.\n",
    "\n",
    "## The game\n",
    "\n",
    "The agent starts in a random empty cell, and has to reach the exit.\n",
    "The exit is randomly positioned in an other empty cell.\n",
    "\n",
    "At each timestep:\n",
    "- the agent decides on an action (move up, left, right or down)\n",
    "- the action is sent to the environment\n",
    "- the environment sends back observations, rewards and a boolean that indicates whether the environment terminated.\n",
    "\n",
    "The environment terminates if the agent reaches the exit, or if the environement reaches a time limit of N^2 timesteps.\n",
    "\n",
    "## Observations\n",
    "\n",
    "The agent receives a dictionary of observations:\n",
    "- target: relative coordinates of the exit \n",
    "- proximity: a 3x3 array that encodes for the value of the cells around the agent.\n",
    "\n",
    "## Rewards\n",
    "\n",
    "When acting, an agent receives a reward depending on the cell it ends up on:\n",
    "- if the agent moves towards an obstacle, it gets a reward of -5 and stays at its original position\n",
    "- if the agent is on a lava cell after its action, it receives a reward of -20\n",
    "- at each timestep, the agent receives an additional reward of -1\n",
    "- when the agent reaches the goal, it receives a reward of N**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Defining the environment.\n",
    "\n",
    "We will define the environment as a class.\n",
    "We are providing pseudo code which is incomplete and probably not completely error-free.\n",
    "\n",
    "You have to fill the blanks.\n",
    "We advise you to look at the pseudo-code for Part 2 and 3 to have an idea of how things work together.\n",
    "\n",
    "In order to make sure that your environment runs as intended, you will create a display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dungeon:\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        \n",
    "        # Numpy array that holds the information about the environment\n",
    "        self.dungeon = ...\n",
    "        \n",
    "        # position of the agent and exit will be decided by resetting the environment.\n",
    "        self.position_agent = None\n",
    "        self.position_exit = None\n",
    "        \n",
    "        # run time\n",
    "        self.time_elapsed = 0\n",
    "        self.time_limit = ...\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        # action is 'up', 'down', 'left', or 'right'\n",
    "        \n",
    "        # modify the position of the agent\n",
    "        \n",
    "        # calculate total reward\n",
    "        \n",
    "        # calculate observations\n",
    "        \n",
    "        # update time\n",
    "        \n",
    "        # verify termination condition\n",
    "        \n",
    "        return observations, reward, done\n",
    "    \n",
    "    def display(self):\n",
    "        \n",
    "        # prints the environment\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This function resets the environment to its original state (time = 0).\n",
    "        Then it places the agent and exit at new random locations.\n",
    "        \n",
    "        It is common practice to return the observations, \n",
    "        so that the agent can decide on the first action right after the resetting of the environment.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # position of the agent is a numpy array\n",
    "        self.position_agent = ...\n",
    "        \n",
    "        # position of the exit is a numpy array\n",
    "        self.position_exit = ...\n",
    "        \n",
    "        # Calculate observations\n",
    "    \n",
    "        return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dungeon = Dungeon(10)\n",
    "dungeon.reset()\n",
    "\n",
    "print(dungeon.position_agent)\n",
    "dungeon.display()\n",
    "obs, reward, done = dungeon.step( ... )\n",
    "\n",
    "print(dungeon.position_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Defining a policy\n",
    "\n",
    "A policy tells the agent how to act depending on its current observation and internal beliefs.\n",
    "\n",
    "As a first simple case, we will define policy as a function that maps observations to actions.\n",
    "\n",
    "As your agent is stupid and doesn't have any way of learning what to do, in this first lab we will write by hand the policy.\n",
    "Try to come up with a strategy to terminate the game with the maximum reward.\n",
    "\n",
    "We advise you to start with a very simple policy, then maybe try a random policy, and finally an 'intelligent' policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(observation):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return action\n",
    "\n",
    "def random_policy(observation):\n",
    "    ...\n",
    "    \n",
    "def intelligent_policy(observation):\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Evaluating your policy\n",
    "\n",
    "Now that you have the environment and policies, you can simulate runs of your games under different policies and evaluate the reward that particular policies will get upon termination of the environment. \n",
    "\n",
    "To that effect, we will create a function run_single_experiment, which will have as input:\n",
    "- an instance of an environment\n",
    "- a policy\n",
    "\n",
    "And it will return the reward obtained once the environment terminates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_exp(envir, policy):\n",
    "    \n",
    "    obs = envir.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = ...\n",
    "        obs, reward, done = ...\n",
    "        \n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Evaluating your policy\n",
    "\n",
    "Now that you can evaluate how a policy performs on a particular environment, consider the following.\n",
    "Because of stochasticity of initial agent position and exit position, different runs will lead to different total rewards.\n",
    "\n",
    "To properly evaluate our policies, we must calculate the statistics over multiple runs.\n",
    "\n",
    "To that effect, we will create a function run_experiments, which will have as input:\n",
    "- an instance of an environment\n",
    "- a policy\n",
    "- a number of times that the experiment will be run\n",
    "\n",
    "It will return the maximum reward obtained over all the runs, the average and variance over the rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(envir, policy, number_exp):\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for n in range(number_exp):\n",
    "        \n",
    "        final_reward = run_single_exp(envir, policy)\n",
    "        all_rewards.append(final_reward)\n",
    "    \n",
    "    max_reward = \n",
    "    mean_reward = \n",
    "    var_reward = \n",
    "    \n",
    "    return max_reward, mean_reward, var_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "\n",
    "Draw some plots to compare how your different policies perform depending on the environment size.\n",
    "\n",
    "As the environment generation is also stochastic (random obstacles and lava), you might need to compute additional statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
